{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96103ed6",
   "metadata": {},
   "source": [
    "# CART (Classification and Regression Tree) Vs. Linear regression\n",
    "\n",
    "The purpose of this exercise is to provide an initial sense on how decision Tree compares to a conventional linear regression when the response variable is quantitative.\n",
    "\n",
    "1.  To begin, construct a regression dataset with known properties:\n",
    "\n",
    "    ```\n",
    "    x1 = np.random.normal(size = 300)\n",
    "    x2 = np.random.normal(size = 300)\n",
    "    error = 2 * np.random.normal(size = 300)\n",
    "    y1 = 1 + (2 * x1) + (3 * x2) + error\n",
    "    ```\n",
    "\n",
    "    - Apply conventional linear regression using. Then fit a Regression Tree (RT), and print it. \n",
    "    - Compare the regression output to the way in which the data were actually generated. \n",
    "    - Compare the tree diagram to the way in which the data were actually generated. \n",
    "    - Compare how well linear regression and regression Tree fit the data. \n",
    "    - What do you conclude about the relative merits of linear regression and RT when the *f(X)* is actually linear and additive ?\n",
    "\n",
    "2. Now, redefine the two predictors as binary factors and reconstruct the response variable:\n",
    "\n",
    "    Transform predictors into binary factors\n",
    "    ```\n",
    "   x11 = (x1 > 0).astype(int)\n",
    "   x22 = (x2 > 0).astype(int)\n",
    "    ```\n",
    "   Reconstruct the response variable\n",
    "    ```\n",
    "   y = 1 + (2 * x11) + (3 * x22) + error\n",
    "    ```\n",
    "    - Repeat the comparison. What do you conclude when the *f(X)* is actually a step function and additive ?\n",
    "\n",
    "3. Under what circumstances is CART likely to perform better than linear regression ? Consider separately the matter of how well the fitted values correspond to the observed values and the implementation of how the predictors are related to the response.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a963716f5b141ec5",
   "metadata": {},
   "source": [
    "# CART and Gini Index\n",
    "\n",
    "\n",
    "### Overview and Dataset\n",
    "In this notebook, we will use a **Decision Tree** classifier to predict the **Tax_Regime** (Exempt, Flat, or Progressive) for various entities (companies/individuals), given their **Country**, **Industry**, **Annual_Income**, and **Number_of_Employees**. \n",
    "\n",
    "We will also explore how **Gini impurity** guides the tree's splitting decisions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880045f2f9922c08",
   "metadata": {},
   "source": [
    "## Part 1: Gini Impurity and Manual Split Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464713b117d47d89",
   "metadata": {},
   "source": [
    "### Task - 1 Compute the Root Gini Impurity\n",
    "    Understand the dataset and find interesting patterns :\n",
    "        - Calculate the overall Gini of the target variable (Tax_Regime) for the entire dataset\n",
    "        - Show your formula and numerical steps for first 10-15 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7424ee2ebe7485",
   "metadata": {},
   "source": [
    "### Task 2 â€“ Split by One Feature\n",
    "    Understand the dataset and find interesting patterns :\n",
    "        - Choose one feature (e.g., Annual_Income, Employees, or a categorical column) to perform a manual split\n",
    "            - If numeric (e.g., Annual_Income): pick a threshold (e.g., s < 60000 vs. s >= 60000)\n",
    "            - If categorical (e.g., Country): pick some subset of countries vs. the others \n",
    "        - Separate the dataset into two subsets and compute the Gini impurity for each subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c462ab0801510df",
   "metadata": {},
   "source": [
    "### Task 3 - Weighted Average Gini After Split\n",
    "\n",
    "\n",
    "$$\n",
    "\n",
    "G(t) = p_l \\cdot G(t_l) + p_r \\cdot G(t_r)\n",
    "\n",
    "$$\n",
    "where :\n",
    "- $  G(t)  $ : Represents the **Gini impurity of the split** at a node $  t  $ .\n",
    "- $  p_l  $ : Proportion of samples in the **left child** ($  t_l  $ ).\n",
    "- $  G(t_l)  $ : Gini impurity of the left child.\n",
    "- $  p_r  $ : Proportion of samples in the **right child** ($  t_r  $ ).\n",
    "- $  G(t_r)  $ : Gini impurity of the right child.\n",
    "\n",
    "\n",
    "\n",
    "Compare this to the root Gini impurity and discuss if the reduction is significant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbd44d542c80821",
   "metadata": {},
   "source": [
    "## Part 2: Building and Evaluating a Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb2b83abc81f2a7",
   "metadata": {},
   "source": [
    "### Task 1 - Data Preparation\n",
    "Prepare the data as usual :\n",
    "\n",
    "    - Do some data analysis (they should be insighful for the problem)\n",
    "    - For categorical features (Country, Industry), convert them to dummy variables\n",
    "    - Split off the target\n",
    "    - Create a 70/30 train-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b3a7cdddaed9e8",
   "metadata": {},
   "source": [
    "### Task 2 - Decision Tree Classifier with Gini\n",
    "    - Initialize and fit\n",
    "    - Tune hyperparameters like max_depth, min_samples_leaf, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d474d99affe05b2e",
   "metadata": {},
   "source": [
    "### Task 3 - Predictions and Evaluation\n",
    "    - Predict on the test set\n",
    "    - Calculate and analyse different classification metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526870f1fe0565d7",
   "metadata": {},
   "source": [
    "### Task 4 - Tree Structure\n",
    "    - Plot and check out how is the tree splitting\n",
    "    - Compare the top splits with your manual Gini analysis from Part 1\n",
    "    - Re train with different max_depth values and compare\n",
    "    - Discuss what are the pro and cons of growing a deep Tree\n",
    "    - Remember what is the variance of an estimator. Discuss why the variance of a decision tree might be large. Find a way to vizualize it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
